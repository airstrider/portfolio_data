{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Tensorflow classification deep learning model\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project will build a neural network model to predict the kind of iris flowers.\n",
    "The goal is to:\n",
    "- Build a neural network model with a provided dataset using Tensorflow packages.\n",
    "- Make predictions of iris using the model.\n",
    "\n",
    "Before building and executing the neural network model, basic EDA, data cleaning, and other manipulations will be conducted to prepare the data for modeling if necessary.\n",
    "\n",
    "Modeling follows the steps:\n",
    "1. Importing packages and loading data\n",
    "2. Exploring the data and completing the cleaning process (optional)\n",
    "3. Building a neural network\n",
    "4. Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing packages and loading data\n",
    "\n",
    "#### 1.1. Import packages\n",
    "\n",
    "Import relevant Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard operational packages\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Modeling and evaluation packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building a neural network model\n",
    "\n",
    "#### 2.1. Create the training and testing data\n",
    "\n",
    "1. Load `iris` dataset from the tensorflow data storage.\n",
    "2. Create a `training` data set by `80%` of the original data.\n",
    "3. Create a `validation` data set by `20%`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tfds.load('iris', split='train[:80%]')\n",
    "valid_dataset = tfds.load('iris', split='train[80%:]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Create a neural network model\n",
    "\n",
    "1. Identify a method to prepare a train set for neural network model.\n",
    "2. Prepare a train set and valid set with the method.\n",
    "3. Set a model check point.\n",
    "4. Create a neural network model.\n",
    "5. Compile the model.\n",
    "6. Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Identify a method to prepare a train set for neural network model.\n",
    "def preprocess(data):\n",
    "    # Should return features and one-hot encoded labels\n",
    "    x = data['features']\n",
    "    y = data['label']\n",
    "    y = tf.one_hot(y, 3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Prepare a train set and valid set with the method.\n",
    "train_set = train_dataset.map(preprocess).batch(10)\n",
    "valid_set = valid_dataset.map(preprocess).batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Set a model check point.\n",
    "checkpoint_path = '../model/temp_checkpoint.ckpt'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                             save_weights_only=True,\n",
    "                             save_best_only=True,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Create a neural network model.\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(4,)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Execute the neural network Model\n",
    "\n",
    "1. Compile the model.\n",
    "2. Fit the model.\n",
    "3. Evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Compile the model.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Fit the model.\n",
    "model.fit(train_set,\n",
    "            validation_data=(valid_set),\n",
    "            epochs=20,\n",
    "            callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1425 - acc: 0.9333\n",
      "evaluate: [0.14245633780956268, 0.9333333373069763]\n"
     ]
    }
   ],
   "source": [
    "#3. Evaluate the results.\n",
    "model.load_weights(checkpoint_path)\n",
    "print('evaluate:', model.evaluate(valid_set))\n",
    "model.save('../model/tensorflow-iris.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rkim_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
